import sys
import os
import platform
import time
import pandas as pd
import threading
import tkinter as tk
from tkinter import ttk, scrolledtext, messagebox
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright

# =================================================================
# [1] ë¸Œë¼ìš°ì € ì„¤ì¹˜ ê²½ë¡œ ì„¤ì • (ê¸°ì¡´ê³¼ ë™ì¼)
# =================================================================
def get_browser_path():
    system_os = platform.system()
    if system_os == 'Darwin':  # Mac
        user_home = os.path.expanduser("~")
        base_path = os.path.join(user_home, "Library", "Application Support", "SmartStoreScraper")
    else:  # Windows
        if getattr(sys, 'frozen', False):
            base_path = os.path.dirname(sys.executable)
        else:
            base_path = os.path.dirname(os.path.abspath(__file__))

    browser_folder = os.path.join(base_path, "browsers")
    try:
        os.makedirs(browser_folder, exist_ok=True)
    except Exception:
        pass
    return browser_folder

BROWSER_FOLDER = get_browser_path()
os.environ["PLAYWRIGHT_BROWSERS_PATH"] = BROWSER_FOLDER

# =================================================================
# [2] (NEW) ê²°ê³¼ íŒŒì¼ ì €ì¥ ê²½ë¡œ ì„¤ì • (ë‹¤ìš´ë¡œë“œ í´ë”ë¡œ ë³€ê²½)
# =================================================================
def get_save_path(filename="reviews.csv"):
    """
    ë§¥ë¶ì˜ 'Read-only file system' ì—ëŸ¬ë¥¼ í”¼í•˜ê¸° ìœ„í•´
    ê²°ê³¼ë¬¼ì„ ë¬´ì¡°ê±´ 'ë‹¤ìš´ë¡œë“œ(Downloads)' í´ë”ì— ì €ì¥í•©ë‹ˆë‹¤.
    """
    user_home = os.path.expanduser("~")
    
    # OS ìƒê´€ì—†ì´ ê·¸ëƒ¥ 'ë‹¤ìš´ë¡œë“œ' í´ë”ì— ì €ì¥í•˜ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì „í•˜ê³  ì°¾ê¸° ì‰¬ì›€
    download_folder = os.path.join(user_home, "Downloads")
    
    # ë‹¤ìš´ë¡œë“œ í´ë”ê°€ ì—†ìœ¼ë©´(í˜¹ì‹œë‚˜), ë°”íƒ•í™”ë©´ìœ¼ë¡œ
    if not os.path.exists(download_folder):
        download_folder = os.path.join(user_home, "Desktop")
        
    return os.path.join(download_folder, filename)

# =================================================================
# [3] ìŠ¤í¬ë¡¤ ê¸°ëŠ¥
# =================================================================
def smooth_scroll(target_frame, steps=10, delay=0.2):
    try:
        for _ in range(steps):
            target_frame.evaluate("window.scrollBy(0, 800)")
            time.sleep(delay)
    except Exception:
        pass

# =================================================================
# [4] GUI í´ë˜ìŠ¤
# =================================================================
class ScraperGUI:
    def __init__(self, root):
        self.root = root
        self.root.title("ë„¤ì´ë²„ ìŠ¤ë§ˆíŠ¸ìŠ¤í† ì–´ ë¦¬ë·° ìˆ˜ì§‘ê¸°")
        self.root.geometry("600x550")
        self.root.resizable(False, False)

        style = ttk.Style()
        style.configure("TLabel", font=("Malgun Gothic", 10))
        style.configure("TButton", font=("Malgun Gothic", 10, "bold"))

        input_frame = ttk.LabelFrame(root, text="ìˆ˜ì§‘ ì„¤ì •", padding=(10, 10))
        input_frame.pack(fill="x", padx=10, pady=10)

        ttk.Label(input_frame, text="ìƒí’ˆ URL:").grid(row=0, column=0, sticky="w", pady=5)
        self.url_entry = ttk.Entry(input_frame, width=50)
        self.url_entry.grid(row=0, column=1, padx=5, pady=5)

        ttk.Label(input_frame, text="ìˆ˜ì§‘ í˜ì´ì§€ ìˆ˜:").grid(row=1, column=0, sticky="w", pady=5)
        self.limit_entry = ttk.Entry(input_frame, width=10)
        self.limit_entry.insert(0, "13") 
        self.limit_entry.grid(row=1, column=1, sticky="w", padx=5, pady=5)

        self.start_btn = ttk.Button(input_frame, text="ìˆ˜ì§‘ ì‹œì‘", command=self.start_thread)
        self.start_btn.grid(row=2, column=0, columnspan=2, pady=10, sticky="ew")

        log_frame = ttk.LabelFrame(root, text="ì§„í–‰ ìƒí™©", padding=(10, 10))
        log_frame.pack(fill="both", expand=True, padx=10, pady=5)

        self.log_text = scrolledtext.ScrolledText(log_frame, height=15, state='disabled', font=("Consolas", 9))
        self.log_text.pack(fill="both", expand=True)

        self.log("í”„ë¡œê·¸ë¨ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì €ì¥ ê²½ë¡œ ë¯¸ë¦¬ ì•ˆë‚´
        save_path = get_save_path()
        self.log(f"ğŸ’¾ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {save_path}")
        self.log("ğŸ‘‰ URLì„ ì…ë ¥í•˜ê³  [ìˆ˜ì§‘ ì‹œì‘]ì„ ëˆ„ë¥´ì„¸ìš”.")

    def log(self, message):
        self.log_text.configure(state='normal')
        self.log_text.insert(tk.END, message + "\n")
        self.log_text.see(tk.END)
        self.log_text.configure(state='disabled')

    def start_thread(self):
        url = self.url_entry.get().strip()
        limit = self.limit_entry.get().strip()

        if not url:
            messagebox.showwarning("ê²½ê³ ", "URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return
        if not limit.isdigit():
            messagebox.showwarning("ê²½ê³ ", "í˜ì´ì§€ ìˆ˜ëŠ” ìˆ«ìë§Œ ì…ë ¥í•´ì£¼ì„¸ìš”!")
            return

        self.start_btn.config(state="disabled")
        self.log("\n[ì‘ì—… ì‹œì‘] --------------------------------")
        
        t = threading.Thread(target=self.run_scraper, args=(url, int(limit)))
        t.daemon = True
        t.start()

    def run_scraper(self, url, limit_pages):
        try:
            self.install_browser_if_needed()
            extract_reviews_to_csv(self, url, limit_pages)
            
            # ì™„ë£Œ ë©”ì‹œì§€ì— ì €ì¥ ê²½ë¡œ í¬í•¨
            save_path = get_save_path()
            messagebox.showinfo("ì™„ë£Œ", f"ìˆ˜ì§‘ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!\níŒŒì¼ ìœ„ì¹˜: {save_path}")
        except Exception as e:
            self.log(f"âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            messagebox.showerror("ì—ëŸ¬", f"ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\n{e}")
        finally:
            self.start_btn.config(state="normal")

    def install_browser_if_needed(self):
        self.log("âš™ï¸ ë¸Œë¼ìš°ì € ì—”ì§„ ìƒíƒœ í™•ì¸ ì¤‘...")
        try:
            with sync_playwright() as p:
                p.chromium.launch(headless=True).close()
            self.log("âœ… ë¸Œë¼ìš°ì € ì—”ì§„ ì •ìƒ.")
        except Exception:
            self.log("ğŸš€ ë¸Œë¼ìš°ì € ì—”ì§„ ìë™ ì„¤ì¹˜ ì‹œì‘ (1~2ë¶„ ì†Œìš”)...")
            try:
                from playwright.__main__ import main
                old_argv = sys.argv
                sys.argv = ["playwright", "install", "chromium"]
                try:
                    main()
                except SystemExit:
                    pass
                finally:
                    sys.argv = old_argv
                self.log("âœ… ë¸Œë¼ìš°ì € ì„¤ì¹˜ ì™„ë£Œ!")
            except Exception as e:
                self.log(f"âŒ ì„¤ì¹˜ ì‹¤íŒ¨: {e}")
                raise e

# =================================================================
# [5] ìŠ¤í¬ë˜í•‘ ë¡œì§
# =================================================================
def parse_review_card(card):
    # (ì´ì „ ì½”ë“œì™€ ë™ì¼ - ìƒëµ)
    nickname_el = card.select_one(".Db9Dtnf7gY strong")
    nickname = nickname_el.get_text(strip=True) if nickname_el else ""
    date_el = card.select_one(".Db9Dtnf7gY span:nth-of-type(1)")
    date = date_el.get_text(strip=True) if date_el else ""
    rating_el = card.select_one("em.n6zq2yy0KA")
    rating = rating_el.get_text(strip=True) if rating_el else ""
    option = ""
    option_box = card.select_one(".b_caIle8kC")
    if option_box:
        all_texts = list(option_box.stripped_strings)
        option = all_texts[0] if all_texts else ""
    buyer_el = card.select_one(".eWRrdDdSzW")
    buyer_info = buyer_el.get_text(" ", strip=True) if buyer_el else ""
    label_el = card.select_one(".h8uqAeqIe7")
    label_info = label_el.get_text(" ", strip=True) if label_el else ""
    auto_label = " | ".join(x for x in [buyer_info, label_info] if x)
    content = ""
    content_box = card.select_one(".KqJ8Qqw082")
    if content_box:
        spans = content_box.select("span")
        if len(spans) >= 2:
            tags = [s.get_text(strip=True) for s in spans[:-1]]
            body = spans[-1].get_text(" ", strip=True)
            content = " ".join(tags + [body])
        elif len(spans) == 1:
            content = spans[0].get_text(" ", strip=True)
    image_count = 0
    img_box = card.select_one(".s30AvhHfb0")
    if img_box:
        count_span = img_box.select_one(".lOzR1kO8jf")
        if count_span:
            number = "".join(c for c in count_span.get_text(strip=True) if c.isdigit())
            if number:
                image_count = int(number)
        else:
            imgs = img_box.select("img")
            if len(imgs) >= 1:
                image_count = 1
    return {
        "nickname": nickname,
        "date": date,
        "rating": rating,
        "option": option,
        "auto_label": auto_label,
        "content": content,
        "image_count": image_count,
    }

def load_review_frame(gui, page):
    gui.log("ğŸ” ë¦¬ë·°íƒ­ íƒìƒ‰ ì¤‘â€¦")
    for _ in range(40):
        btn = page.locator('[data-name="REVIEW"]').first
        if btn.is_visible():
            btn.scroll_into_view_if_needed()
            time.sleep(0.5)
            btn.click()
            gui.log("âœ” ë¦¬ë·°íƒ­ í´ë¦­ ì„±ê³µ")
            break
        page.mouse.wheel(0, 600)
        time.sleep(0.2)
    else:
        gui.log("âŒ ë¦¬ë·°íƒ­ ëª» ì°¾ìŒ.")
        return page

    gui.log("âŒ› ë¦¬ë·° iframe ë¡œë”© ëŒ€ê¸°â€¦")
    for _ in range(80):
        for f in page.frames:
            lower = f.url.lower()
            if ("review" in lower) or ("reviews" in lower) or ("pstatic" in lower):
                gui.log(f"âœ” iframe ê°ì§€ë¨")
                return f
        time.sleep(0.25)
    return page

def load_next_page(gui, target_frame, current_page_num):
    next_page_num = current_page_num + 1
    next_btn = target_frame.locator(f'.LiT9lKOVbw a:has-text("{next_page_num}")').first
    if next_btn.count() > 0:
        gui.log(f"â¡ í˜ì´ì§€ {next_page_num} ì´ë™")
        next_btn.click()
        time.sleep(2)
        return True
    else:
        return False

def extract_reviews_to_csv(gui, url, limit_pages=13):
    reviews = []
    seen = set()

    with sync_playwright() as p:
        browser = p.chromium.launch(headless=False)
        page = browser.new_page()
        gui.log(f"â³ í˜ì´ì§€ ì ‘ì† ì¤‘: {url}")
        
        try:
            page.goto(url, timeout=60000)
        except Exception:
            gui.log("âš ï¸ ì ‘ì† ì§€ì—° (ê³„ì† ì§„í–‰)")

        time.sleep(3)
        target_frame = load_review_frame(gui, page)
        
        if target_frame is page and target_frame.url == url and page.locator(".IwcuBUIAKf").count() == 0:
            gui.log("âŒ ë¦¬ë·° ì„¹ì…˜ ë¡œë“œ ì‹¤íŒ¨.")
            browser.close()
            return
            
        for n in range(1, limit_pages + 1):
            gui.log(f"ğŸ“Œ í˜ì´ì§€ {n} ìˆ˜ì§‘ ì¤‘â€¦")
            gui.log("   (ìŠ¤í¬ë¡¤ ë‚´ë¦¬ëŠ” ì¤‘...)")
            smooth_scroll(target_frame, steps=10, delay=0.2)

            soup = BeautifulSoup(target_frame.content(), "lxml")
            review_cards = soup.select(".IwcuBUIAKf")
            
            current_page_reviews = 0
            for card in review_cards:
                info = parse_review_card(card)
                if not info: continue
                
                key = f"{info['nickname']}|{info['date']}|{info['content'][:10]}"
                if key not in seen:
                    seen.add(key)
                    reviews.append(info)
                    current_page_reviews += 1

            gui.log(f"   â”” ì‹ ê·œ: {current_page_reviews}ê±´ (ëˆ„ì : {len(reviews)}ê±´)")
            
            if not load_next_page(gui, target_frame, n):
                gui.log("â›” ë‹¤ìŒ í˜ì´ì§€ ì—†ìŒ")
                break

        browser.close()

    # [í•µì‹¬ ë³€ê²½] ì €ì¥ ê²½ë¡œë¥¼ ë‹¤ìš´ë¡œë“œ í´ë”ë¡œ ì§€ì •
    save_path = get_save_path("reviews.csv")
    
    df = pd.DataFrame(reviews)
    df.to_csv(save_path, index=False, encoding="utf-8-sig")
    
    gui.log("====================================")
    gui.log(f"âœ… ì´ {len(reviews)}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")
    gui.log(f"ğŸ“ íŒŒì¼ ì €ì¥ ì™„ë£Œ: {save_path}") # ë¡œê·¸ì—ë„ ê²½ë¡œ í‘œì‹œ

if __name__ == "__main__":
    root = tk.Tk()
    app = ScraperGUI(root)
    root.mainloop()
